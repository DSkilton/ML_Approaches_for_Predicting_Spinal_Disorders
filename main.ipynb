{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Create Accuracy score class\n",
    "# TODO: Investigate dimensionality reduction using PCA\n",
    "# TODO: Investigate training on images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:25:44.552421Z",
     "iopub.status.busy": "2025-04-01T15:25:44.552421Z",
     "iopub.status.idle": "2025-04-01T15:25:49.531271Z",
     "shell.execute_reply": "2025-04-01T15:25:49.531271Z"
    }
   },
   "outputs": [],
   "source": [
    "# All required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "\n",
    "from constants import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer # This is required for IterativeImputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "sys.path.append(os.getcwd()) \n",
    "from constants import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:25:49.534280Z",
     "iopub.status.busy": "2025-04-01T15:25:49.533281Z",
     "iopub.status.idle": "2025-04-01T15:25:49.554280Z",
     "shell.execute_reply": "2025-04-01T15:25:49.554280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pelvic_incidence</th>\n",
       "      <th>pelvic_tilt</th>\n",
       "      <th>lumbar_lordosis_angle</th>\n",
       "      <th>sacral_slope</th>\n",
       "      <th>pelvic_radius</th>\n",
       "      <th>degree_spondylolisthesis</th>\n",
       "      <th>class</th>\n",
       "      <th>pelvic_slope</th>\n",
       "      <th>direct_tilt</th>\n",
       "      <th>thoracic_slope</th>\n",
       "      <th>cervical_tilt</th>\n",
       "      <th>sacrum_angle</th>\n",
       "      <th>scoliosis_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.027817</td>\n",
       "      <td>22.552586</td>\n",
       "      <td>39.609117</td>\n",
       "      <td>40.475232</td>\n",
       "      <td>98.672917</td>\n",
       "      <td>-0.254400</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.056951</td>\n",
       "      <td>10.060991</td>\n",
       "      <td>25.015378</td>\n",
       "      <td>28.995960</td>\n",
       "      <td>114.405425</td>\n",
       "      <td>4.564259</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.832021</td>\n",
       "      <td>22.218482</td>\n",
       "      <td>50.092194</td>\n",
       "      <td>46.613539</td>\n",
       "      <td>105.985135</td>\n",
       "      <td>-3.530317</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.297008</td>\n",
       "      <td>24.652878</td>\n",
       "      <td>44.311238</td>\n",
       "      <td>44.644130</td>\n",
       "      <td>101.868495</td>\n",
       "      <td>11.211523</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.712859</td>\n",
       "      <td>9.652075</td>\n",
       "      <td>28.317406</td>\n",
       "      <td>40.060784</td>\n",
       "      <td>108.168725</td>\n",
       "      <td>7.918501</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
       "0         63.027817    22.552586              39.609117     40.475232   \n",
       "1         39.056951    10.060991              25.015378     28.995960   \n",
       "2         68.832021    22.218482              50.092194     46.613539   \n",
       "3         69.297008    24.652878              44.311238     44.644130   \n",
       "4         49.712859     9.652075              28.317406     40.060784   \n",
       "\n",
       "   pelvic_radius  degree_spondylolisthesis   class  pelvic_slope  direct_tilt  \\\n",
       "0      98.672917                 -0.254400  Hernia           NaN          NaN   \n",
       "1     114.405425                  4.564259  Hernia           NaN          NaN   \n",
       "2     105.985135                 -3.530317  Hernia           NaN          NaN   \n",
       "3     101.868495                 11.211523  Hernia           NaN          NaN   \n",
       "4     108.168725                  7.918501  Hernia           NaN          NaN   \n",
       "\n",
       "   thoracic_slope  cervical_tilt  sacrum_angle  scoliosis_slope  \n",
       "0             NaN            NaN           NaN              NaN  \n",
       "1             NaN            NaN           NaN              NaN  \n",
       "2             NaN            NaN           NaN              NaN  \n",
       "3             NaN            NaN           NaN              NaN  \n",
       "4             NaN            NaN           NaN              NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "unclean_data = pd.read_csv('./data_set/combined.csv')\n",
    "unclean_data.head()\n",
    "# unclean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has 6 columns with approximately half having accurate float information and the remaining half is recorded as NaN. There are several options we can explore. \n",
    "\n",
    "1. We can use the data while removing the last 6 columns leaving 3000 rows of data and 7 columns. For a preliminary analysis we can drop the features initially. \n",
    "\n",
    "2. We can keep only complete records of data which will leave 1500 rows of data with 13 columns. Although this improves reliability, it is at the cost of reducing the sample size.\n",
    "\n",
    "3. Lastly we can impute the missing information. In spinal biomechanics, there are mathematical interrelated. Pelvic Incidence, Sacral Slope and Pelvic Tilt are particularly useful because it can be demonstrated that PI is the arithmetic sum of the sacral slope (SS) pelvic tilt (PT), the two position-dependent variables that determine pelvic orientation in the sagittal plane (Labelle et al., 2005). Normalization will also be applied to mitigate issues with feature scaling. (Jolliffe, 2011)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Missing Value Imputation</h1>\n",
    "We will implement several imputation methods and evaluate their effects on the dataset. \n",
    "\n",
    "<strong>Mean Imputation:</strong> Missing values can be replaced with the mean of the feature. this is simple but can be influenced by outliers, potentially skewing the distribution. It can reduce variance since all missing entries are filled with an average value (C. Hounmenou, 2024)\n",
    "\n",
    "<strong>Median Imputation:</strong> Replace missing values with the median of the feature. Median tends to be more robust to outliers then the mean, so this method preserves the central tenancy without being dragged by extrem values. In practice, mean and median can often yield similar results for symmetric distributions, but median imputation can better maintain the distribution shape when the data is skewed or contains outliers (M. Badiaa, 2024).\n",
    "\n",
    "<strong>K-Nearest Neighbours Imputation (KNN):</strong> Using KNN estimates missing values using the mean, or median, of the <i>k</i> closest records, the feature space, that have values for that feature. This method leverages relationships between features, under the assumption that similar samples have similar feature values. By considering the distance in all dimensions, KNN imputation can preserve the multivariate structure of the data, often leading to more plausible imputed values (T. Kaur, 2025). \n",
    "\n",
    "<strong>Regression-based Imputation (Iterative/MICE):</strong> Uses regression models in an iterative fashion to predict missing values. Methods like MICE, Multiple Imputation by Chained Equations, or MissForest, treat each feature with missing data as a target and learn a regression, or decision tree, model from other features to predict the missing value (Y. Sun et al, 2023). This approach takes into account dependencies between variables and often yields higher quality imputations than mean/ median filling. It learns the likely value of a feature based on rathers, rather than inserting a constant value (T. Lekhansh, 2024). \n",
    "\n",
    "<h1>Comparison of Imputation Techniques</h1> \n",
    "After imputing missing values using each method, we will examine how they affect the data distribution and model performance. \n",
    "\n",
    "<strong>Distribution Effects:</strong> We will visualise and summarise the distribution of key features before and after imputation. For example, histograms and density plots will be compared to see if the mean or median imputation significantly shifts the distribution or compresses variability. \n",
    "\n",
    "Mean imputation may underestimate variance, since many values become the mean, whereas KNN or regression imputation may preserve the natural variance by inferring different values for different samples. We'll also check if any imputation methods might preserve relationships between feature, whereas mean/ median could break those correlations. \n",
    "\n",
    "<strong>Model Performance:</strong> We will train a baseline machine learning model, such as a logistic regression and compare their accuracy before and after imputation. The intuition is that a better imputation method provides data closer to the truth, potentially improving model accuracy. For example, median imputation might improve robustness on datasets with outliers, and more sophisticated approaches like KNN or MICE often yield higher predictive performance (lower error) since they utilise additional information from other features (S. Alam, 2023). By evaluating the model’s accuracy after each imputation, we can quantify the impact of each cleaning method. Any improvement in accuracy or reduction in error when using KNN/MICE over simple mean imputation would indicate that preserving multivariate structure is beneficial for this dataset.\n",
    "\n",
    "Throughout this process, careful cross-validation will be used to ensure that the comparisons are fair, so differences in performance are due to imputation rather than random train test splits or variance in random state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:25:49.579743Z",
     "iopub.status.busy": "2025-04-01T15:25:49.579743Z",
     "iopub.status.idle": "2025-04-01T15:25:49.646604Z",
     "shell.execute_reply": "2025-04-01T15:25:49.645600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputation Method: Mean\n",
      "pelvic_slope: MSE = 0.079788, R2 = -0.005898\n",
      "direct_tilt: MSE = 69.750749, R2 = -0.002271\n",
      "thoracic_slope: MSE = 11.989663, R2 = -0.026355\n",
      "cervical_tilt: MSE = 9.069290, R2 = -0.003769\n",
      "sacrum_angle: MSE = 165.943090, R2 = -0.010477\n",
      "scoliosis_slope: MSE = 106.131652, R2 = -0.000737\n",
      "\n",
      "Imputation Method: Median\n",
      "pelvic_slope: MSE = 0.079863, R2 = -0.006842\n",
      "direct_tilt: MSE = 71.153964, R2 = -0.022434\n",
      "thoracic_slope: MSE = 11.909108, R2 = -0.019459\n",
      "cervical_tilt: MSE = 9.065999, R2 = -0.003405\n",
      "sacrum_angle: MSE = 164.572917, R2 = -0.002134\n",
      "scoliosis_slope: MSE = 107.080517, R2 = -0.009684\n",
      "\n",
      "Imputation Method: KNN\n",
      "pelvic_slope: MSE = 0.079788, R2 = -0.005898\n",
      "direct_tilt: MSE = 69.750749, R2 = -0.002271\n",
      "thoracic_slope: MSE = 11.989663, R2 = -0.026355\n",
      "cervical_tilt: MSE = 9.069290, R2 = -0.003769\n",
      "sacrum_angle: MSE = 165.943090, R2 = -0.010477\n",
      "scoliosis_slope: MSE = 106.131652, R2 = -0.000737\n",
      "\n",
      "Imputation Method: Regression\n",
      "pelvic_slope: MSE = 0.079788, R2 = -0.005898\n",
      "direct_tilt: MSE = 69.750749, R2 = -0.002271\n",
      "thoracic_slope: MSE = 11.989663, R2 = -0.026355\n",
      "cervical_tilt: MSE = 9.069290, R2 = -0.003769\n",
      "sacrum_angle: MSE = 165.943090, R2 = -0.010477\n",
      "scoliosis_slope: MSE = 106.131652, R2 = -0.000737\n"
     ]
    }
   ],
   "source": [
    "columns_to_impute = ['pelvic_slope', 'direct_tilt', 'thoracic_slope',\n",
    "                      'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']\n",
    "\n",
    "# Select rows without any missing data for evaluation\n",
    "complete_rows = unclean_data.dropna(subset=columns_to_impute).copy()\n",
    "\n",
    "# Artificially create missingness (use 20% of rows for evaluation)\n",
    "X_train, X_eval = train_test_split(complete_rows, test_size=0.2, random_state=FORTY_TWO)\n",
    "\n",
    "# Keep a copy of original evaluation values for comparison\n",
    "X_eval_original = X_eval.copy()\n",
    "\n",
    "# Artificially introduce missingness in evaluation set\n",
    "for col in columns_to_impute:\n",
    "    X_eval.loc[:, col] = np.nan\n",
    "\n",
    "# Imputers dictionary\n",
    "imputers = {\n",
    "    'Mean': SimpleImputer(strategy='mean'),\n",
    "    'Median': SimpleImputer(strategy='median'),\n",
    "    'KNN': KNNImputer(n_neighbors=5),\n",
    "    'Regression': IterativeImputer(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate imputation methods\n",
    "for method_name, imputer in imputers.items():\n",
    "    # Fit imputer on training data\n",
    "    imputer.fit(X_train[columns_to_impute])\n",
    "\n",
    "    # Impute missing values on evaluation set\n",
    "    X_eval_imputed = X_eval.copy()\n",
    "    X_eval_imputed[columns_to_impute] = imputer.transform(X_eval_imputed[columns_to_impute])\n",
    "\n",
    "    print(f\"\\nImputation Method: {method_name}\")\n",
    "\n",
    "    # Calculate MSE and R² for each column\n",
    "    for col in columns_to_impute:\n",
    "        mse = mean_squared_error(X_eval_original[col], X_eval_imputed[col])\n",
    "        r2 = r2_score(X_eval_original[col], X_eval_imputed[col])\n",
    "\n",
    "        print(f\"{col}: MSE = {mse:.6f}, R2 = {r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalising the data</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:25:49.649608Z",
     "iopub.status.busy": "2025-04-01T15:25:49.648611Z",
     "iopub.status.idle": "2025-04-01T15:25:51.690135Z",
     "shell.execute_reply": "2025-04-01T15:25:51.690135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting missing values for: pelvic_slope\n",
      "MSE on test data: 0.001829\n",
      "Accuracy (R² score) on test data: 0.9769\n",
      "Filled 1550 missing values for 'pelvic_slope'.\n",
      "\n",
      "Predicting missing values for: direct_tilt\n",
      "MSE on test data: 2.386533\n",
      "Accuracy (R² score) on test data: 0.9657\n",
      "Filled 1550 missing values for 'direct_tilt'.\n",
      "\n",
      "Predicting missing values for: thoracic_slope\n",
      "MSE on test data: 0.731574\n",
      "Accuracy (R² score) on test data: 0.9374\n",
      "Filled 1550 missing values for 'thoracic_slope'.\n",
      "\n",
      "Predicting missing values for: cervical_tilt\n",
      "MSE on test data: 0.134232\n",
      "Accuracy (R² score) on test data: 0.9851\n",
      "Filled 1550 missing values for 'cervical_tilt'.\n",
      "\n",
      "Predicting missing values for: sacrum_angle\n",
      "MSE on test data: 2.732096\n",
      "Accuracy (R² score) on test data: 0.9834\n",
      "Filled 1550 missing values for 'sacrum_angle'.\n",
      "\n",
      "Predicting missing values for: scoliosis_slope\n",
      "MSE on test data: 1.249659\n",
      "Accuracy (R² score) on test data: 0.9882\n",
      "Filled 1550 missing values for 'scoliosis_slope'.\n",
      "\n",
      "Final missing values after imputation:\n",
      "pelvic_slope       0\n",
      "direct_tilt        0\n",
      "thoracic_slope     0\n",
      "cervical_tilt      0\n",
      "sacrum_angle       0\n",
      "scoliosis_slope    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Columns to predict/impute\n",
    "columns_to_predict = ['pelvic_slope', 'direct_tilt', 'thoracic_slope',\n",
    "                      'cervical_tilt', 'sacrum_angle', 'scoliosis_slope']\n",
    "\n",
    "# Identify predictor columns (excluding columns to impute and 'class')\n",
    "predictor_columns = unclean_data.columns.drop(columns_to_impute + ['class'])\n",
    "\n",
    "for target_column in columns_to_predict:\n",
    "    print(f\"\\nPredicting missing values for: {target_column}\")\n",
    "\n",
    "    # Rows with available data in target column\n",
    "    complete_rows = unclean_data.dropna(subset=[target_column])\n",
    "\n",
    "    X = complete_rows[predictor_columns]\n",
    "    y = complete_rows[target_column]\n",
    "\n",
    "    # Train-test split on complete data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=FORTY_TWO)\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=100, random_state=FORTY_TWO)\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on test set from complete data\n",
    "    y_pred = rf_regressor.predict(X_test)\n",
    "    mse_complete = mean_squared_error(y_test, y_pred)\n",
    "    accuracy_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"MSE on test data: {mse_complete:.6f}\")\n",
    "    print(f\"Accuracy (R² score) on test data: {accuracy_r2:.4f}\")\n",
    "\n",
    "    # Rows with missing target values (but predictor columns filled)\n",
    "    rows_to_predict = unclean_data[unclean_data[target_column].isnull() & \n",
    "                                   unclean_data[predictor_columns].notnull().all(axis=1)]\n",
    "\n",
    "    # Predict missing values\n",
    "    if not rows_to_predict.empty:\n",
    "        predicted_values = rf_regressor.predict(rows_to_predict[predictor_columns])\n",
    "        unclean_data.loc[rows_to_predict.index, target_column] = predicted_values\n",
    "        print(f\"Filled {len(predicted_values)} missing values for '{target_column}'.\")\n",
    "    else:\n",
    "        print(f\"No suitable rows found to predict '{target_column}'.\")\n",
    "\n",
    "# Final check for missing values\n",
    "print(f\"\\nFinal missing values after imputation:\")\n",
    "print(unclean_data[columns_to_impute].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:25:51.693147Z",
     "iopub.status.busy": "2025-04-01T15:25:51.692151Z",
     "iopub.status.idle": "2025-04-01T15:25:53.842440Z",
     "shell.execute_reply": "2025-04-01T15:25:53.842440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Imputation - MSE: 0.000178\n",
      "Median Imputation - MSE: 0.000178\n",
      "KNN Imputation - MSE: 0.000178\n",
      "Regression Imputation - MSE: 0.000178\n"
     ]
    }
   ],
   "source": [
    "# Predictor and target\n",
    "predictors = unclean_data.columns.drop(columns_to_impute + ['class'])\n",
    "target = 'pelvic_slope' \n",
    "\n",
    "mean_imputed_df = unclean_data.copy()\n",
    "median_imputed_df = unclean_data.copy()\n",
    "knn_imputed_df = unclean_data.copy()\n",
    "regression_imputed_df = unclean_data.copy()\n",
    "\n",
    "datasets = {\n",
    "    'Mean': mean_imputed_df,\n",
    "    'Median': median_imputed_df,\n",
    "    'KNN': knn_imputed_df,\n",
    "    'Regression': regression_imputed_df\n",
    "}\n",
    "\n",
    "mse_scores = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    X = df[predictors]\n",
    "    y = df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    preds = rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "\n",
    "    mse_scores[name] = mse\n",
    "    print(f\"{name} Imputation - MSE: {mse:.6f}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:25:53.844709Z",
     "iopub.status.busy": "2025-04-01T15:25:53.844709Z",
     "iopub.status.idle": "2025-04-01T15:26:02.819619Z",
     "shell.execute_reply": "2025-04-01T15:26:02.819619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1477 - mse: 0.1477 - val_loss: 0.0827 - val_mse: 0.0827\n",
      "Epoch 2/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0733 - mse: 0.0733 - val_loss: 0.0744 - val_mse: 0.0744\n",
      "Epoch 3/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0664 - mse: 0.0664 - val_loss: 0.0677 - val_mse: 0.0677\n",
      "Epoch 4/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0619 - mse: 0.0619 - val_loss: 0.0659 - val_mse: 0.0659\n",
      "Epoch 5/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0596 - mse: 0.0596 - val_loss: 0.0583 - val_mse: 0.0583\n",
      "Epoch 6/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0559 - mse: 0.0559 - val_loss: 0.0544 - val_mse: 0.0544\n",
      "Epoch 7/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0513 - mse: 0.0513 - val_loss: 0.0529 - val_mse: 0.0529\n",
      "Epoch 8/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0487 - mse: 0.0487 - val_loss: 0.0501 - val_mse: 0.0501\n",
      "Epoch 9/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0458 - mse: 0.0458 - val_loss: 0.0463 - val_mse: 0.0463\n",
      "Epoch 10/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.0505 - val_mse: 0.0505\n",
      "Epoch 11/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0417 - val_mse: 0.0417\n",
      "Epoch 12/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 13/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0462 - val_mse: 0.0462\n",
      "Epoch 14/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0368 - val_mse: 0.0368\n",
      "Epoch 15/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0333 - val_mse: 0.0333\n",
      "Epoch 16/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0292 - mse: 0.0292 - val_loss: 0.0351 - val_mse: 0.0351\n",
      "Epoch 17/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 18/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0281 - val_mse: 0.0281\n",
      "Epoch 19/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.0258 - val_mse: 0.0258\n",
      "Epoch 20/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 21/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.0216 - val_mse: 0.0216\n",
      "Epoch 22/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.0214 - val_mse: 0.0214\n",
      "Epoch 23/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 24/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0317 - val_mse: 0.0317\n",
      "Epoch 25/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0246 - val_mse: 0.0246\n",
      "Epoch 26/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.0195 - val_mse: 0.0195\n",
      "Epoch 27/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0203 - val_mse: 0.0203\n",
      "Epoch 28/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.0153 - val_mse: 0.0153\n",
      "Epoch 29/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0161 - val_mse: 0.0161\n",
      "Epoch 30/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 31/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 32/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 33/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 34/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 35/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 36/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 37/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0276 - val_mse: 0.0276\n",
      "Epoch 38/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 39/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 40/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 41/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 42/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 43/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 44/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0067 - val_mse: 0.0067\n",
      "Epoch 45/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 46/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 47/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 48/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 49/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 50/50\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0062 - val_mse: 0.0062\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0066 - mse: 0.0066 \n",
      "Neural Network MSE (Regression Imputed Data): 0.006229\n"
     ]
    }
   ],
   "source": [
    "# Choose your preferred imputed dataset for Neural Network\n",
    "nn_df = regression_imputed_df.copy()\n",
    "\n",
    "X = nn_df[predictors]\n",
    "y = nn_df[target]\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural Network Model\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # regression problem, no sigmoid\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Training\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate MSE\n",
    "nn_mse = model.evaluate(X_test, y_test)[1]\n",
    "print(f\"Neural Network MSE (Regression Imputed Data): {nn_mse:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mse_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmse_scores\u001b[49m\u001b[43m)\u001b[49m.T  \u001b[38;5;66;03m# Transpose to get methods as rows\u001b[39;00m\n\u001b[32m      2\u001b[39m sns.heatmap(mse_df, annot=\u001b[38;5;28;01mTrue\u001b[39;00m, cmap=\u001b[33m\"\u001b[39m\u001b[33mYlGnBu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mMSE per Feature by Imputation Method\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Student\\Documents\\MSc\\Machine Learning ICA1\\venv311\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Student\\Documents\\MSc\\Machine Learning ICA1\\venv311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Student\\Documents\\MSc\\Machine Learning ICA1\\venv311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Student\\Documents\\MSc\\Machine Learning ICA1\\venv311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:667\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    664\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIf using all scalar values, you must pass an index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[32m    670\u001b[39m     index = union_indexes(indexes)\n",
      "\u001b[31mValueError\u001b[39m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "mse_df = pd.DataFrame(mse_scores).T\n",
    "sns.heatmap(mse_df, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title(\"MSE per Feature by Imputation Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA, UMAP do we need to use sigmoid function  \n",
    "\n",
    "Having ensured the data is clean and complete, the next step is to reduce its dimensionality to simplify the feature space.\n",
    "\n",
    "<h1>2. Dimensionality Reduction</h1>\n",
    "We will explore 3 dimensionality reduction techqniques Linear Discriminant Analysis (LDA), t-distributed Stochastic Neighbour Embedding (t-SNE), and Principal Component Analysis (PCA), to reduce feature dimensions and visualise the data structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ensemble Learning</h1> \n",
    "\n",
    "Ensemble learning is a technique that combines multiple models to produce a single, more robust prediction (Murel & Kavlakoglu, 2024). Rather than relying on a single model, ensemble methods aggregate the outputs of each model. Each model may capture different aspects or patterns in the data, this approach can have several advantages: \n",
    "\n",
    "<strong>Reduced variance:</strong> By averaging across models, ensemble methods can reduce the variance associated with indiviudal preductions, which oftens leads to imrpoved generalisation on unseen data.\n",
    "\n",
    "<strong>Improved accuracy:</strong> Models like like Bagging, Boosting or Voting classifiers often outperform single models because they compensate for the weaknesses of one algorithm with the stregths of another (GeeksForGeeks, 2024).\n",
    "\n",
    "<strong>Robustness:</strong> Ensembles are less sensitive to the noise associated with training data since outliers are diminished when combining multiple models (A. Jain, 2024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Chosen Models</h1>\n",
    "\n",
    "In Comparison of machine learning algorithms to identify and prevent low back injury by C. Paulino & J. Correa they analyse 6 algorithm types and have the most success with Support Vector Machine (SVM) and K-Nearest Neighbour (KNN). Their data set had under 200 rows of data and received accuracy scores over 90. \n",
    "\n",
    "For this research, we will be using Logistic Regression, Decision Tree Classifier, Random Forest Regressor, XGBoost, LightGBM as well as SWM and KNN. \n",
    "\n",
    "consider neural network, look at activation, sigmoid, epoch, loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Option 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this option we will clean the data by removing the following headings 'pelvic_slope, direct_tilt, thoracic_slope, cervical_tilt, sacrum_angle, scoliosis_slope'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Parameter Tuning and Its Impact</h1>\n",
    "Parameter tuning (or hyperparameter optimisation) is a crucial step in machine learning that can directly impact the model and its performance. Hyperparameters, such as the number of trees (n_estimators), maximum depth (max_depth), and minimum samples required for a split (min_samples_split) in a Random Forest, determine the model's complexity and learning capacity. The following are key several points regarding parameter tuning:\n",
    "\n",
    "<strong>Model complexity:</strong> For instance, setting max_depth too low might result in a model that underfits (i.e. too simple to capture the underlying structure), while setting it too high may lead to overfitting (i.e. the model learns noise in the training data) (Amazon, 2024)\n",
    "\n",
    "<strong>Bias-variance tradeoff:</strong>\n",
    "Parameters such as n_estimators in ensemble models help manage the bias-variance tradeoff. A higher number of trees generally reduces variance but can increase computational cost (S. Prajapati, 2024).\n",
    "\n",
    "<strong>Search techniques:</strong> Techniques like Grid Search and Randomised Search systematically explore combinations of hyperparameters. When paird with cross-validation, these techniques provide a robust estimate of how different parameter configurations perform on unseen data (A. Gupta, 2025). \n",
    "\n",
    "<strong>Performance comparison:</strong> By printing out the results from the hyperparameter grid, we can compare different configurations and indentify the best performing combination. This transparency is essential for reproducibility and for providing insights into why certain parameters work better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:26:02.822626Z",
     "iopub.status.busy": "2025-04-01T15:26:02.821637Z",
     "iopub.status.idle": "2025-04-01T15:27:14.604040Z",
     "shell.execute_reply": "2025-04-01T15:27:14.604040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "columns_to_keep = [\n",
    "    PELVIC_INCIDENCE, PELVIC_TILT, LUMBAR_LORDOSIS_ANGLE, SACRAL_SLOPE, PELVIC_RADIUS, DEGREE_SPONDYLOLISTHESIS, PELVIC_SLOPE, DIRECT_TILT, THORACIC_SLOPE, CERVICAL_TILT, SACRUM_ANGLE, SCOLIOSIS_SLOPE\n",
    "]\n",
    "\n",
    "cleaned_data = unclean_data[columns_to_keep].copy()\n",
    "X_cleaned = cleaned_data.drop(columns=[PELVIC_SLOPE, DIRECT_TILT,THORACIC_SLOPE, CERVICAL_TILT, SACRUM_ANGLE, SCOLIOSIS_SLOPE])\n",
    "y_cleaned = cleaned_data[DEGREE_SPONDYLOLISTHESIS]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.2, random_state=FORTY_TWO)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "forest_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 6, 10, None],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(max_features='sqrt', min_samples_split=2, n_estimators=100, random_state=FORTY_TWO)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(rf, forest_params, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search\n",
    "\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Things to include</h1>\n",
    "Include shap explanation as well as coefficient \n",
    "Check the available shap plots, maybe better than coefficient\n",
    "\n",
    "use a param grid where applicable to ensure we have the best params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>References</H1>\n",
    "\n",
    "(Labelle et al., 2005) - Labelle, H., Roussouly, P., Berthonnaud, É., Dimnet, J. and O’Brien, M. (2005) ‘The importance of spino-pelvic balance in L5–S1: Developmental spondylolisthesis – A review of pertinent radiologic measurements’, Spine, 30(S6), pp. S27–S34.\n",
    "\n",
    "Hounmenou, C. (2024) ‘Difference between outlier & skewness and how to control them in a model’, LinkedIn, Available at: https://www.linkedin.com/pulse/difference-between-outlier-skewness-how-control-them-model-hounmenou-mksre/ (Accessed: 19 March 2025).\n",
    "\n",
    "Badiaa, M. (2024) ‘Discussion on handling missing data in machine learning’, Kaggle, Available at: https://www.kaggle.com/discussions/questions-and-answers/477356 (Accessed: 19 March 2025).\n",
    "\n",
    "Kaur, T. (2025) ‘KNN imputation: The complete guide’, Medium, Available at: https://medium.com/@tarangds/knn-imputation-the-complete-guide-146f932870a7 (Accessed: 19 March 2025).\n",
    "\n",
    "Sun, Y., Li, J., Xu, Y., Zhang, T. and Wang, X. (2023) ‘Deep learning versus conventional methods for missing data imputation: A review and comparative study’, Expert Systems with Applications, Available at: https://www.sciencedirect.com/science/article/pii/S0957417423007030 (Accessed: 19 March 2025).\n",
    "\n",
    "Lekhansh, T. (2024) ‘Handling missing values in machine learning: Strategies for imputation and model robustness’, Medium, Available at: https://medium.com/@tyagi.lekhansh/handling-missing-values-in-machine-learning-strategies-for-imputation-and-model-robustness-4ba6287f1094 (Accessed: 19 March 2025).\n",
    "\n",
    "Alam, S. (2023) ‘Missing value imputation in machine learning: A comparative study’, ScienceDirect, Available at: https://www.sciencedirect.com/science/article/pii/S2772662223001819 (Accessed: 19 March 2025).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T15:27:14.606309Z",
     "iopub.status.busy": "2025-04-01T15:27:14.606309Z",
     "iopub.status.idle": "2025-04-01T15:27:14.609481Z",
     "shell.execute_reply": "2025-04-01T15:27:14.609481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', '.venv', '2019AlaAlKafriPhD.pdf', 'constants.py', 'data_set', 'end-to-end-Ml project', 'ICA-Specification-Machine-Learning.pdf', 'images', 'main.ipynb', 'models', 'persistence', 'project.ipynb', 'requirements.txt', 'scraper.py', 'text_processing', 'venv', 'venv311', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
